{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeca234",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-24T12:15:25.582Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_lg\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3211ceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:23:11.490643Z",
     "start_time": "2022-02-24T12:23:04.843370Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ca1b56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:23:11.589776Z",
     "start_time": "2022-02-24T12:23:11.565491Z"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2vec = en_core_web_lg.load()\n",
    "        self.spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    def tokenize(self, df: pd.Series):\n",
    "        \"\"\"Returns pd.Series of word lists\"\"\"\n",
    "        words = []\n",
    "        for doc in df.values:\n",
    "            doc = self.spacy_nlp(doc)\n",
    "            tokens = [token.text for token in doc]\n",
    "            words.append(tokens)\n",
    "        return pd.Series(words)\n",
    "    \n",
    "    def lemmatization_and_stop_words(self, df: pd.Series):\n",
    "        docs = list(self.spacy_nlp.pipe(X_train))\n",
    "        data_clean = [[w.lemma_ for w in doc if (not w.is_stop and not w.is_punct and not w.like_num)] for doc in docs]\n",
    "        df = pd.Series([' '.join(comment)  for comment in data_clean])\n",
    "        return df\n",
    "    \n",
    "    def get_vec(self, x):\n",
    "        \"\"\"Word2vec\"\"\"\n",
    "        doc = self.word2vec(x)\n",
    "        vec = doc.vector\n",
    "        return vec\n",
    "    \n",
    "    def sen_to_vec(self, sentence, vec_dim = 300):\n",
    "        \"\"\"Simple averaging for word vectors\"\"\"\n",
    "        ans = np.zeros(vec_dim)\n",
    "        for word in sentence:\n",
    "            ans += self.get_vec(word)\n",
    "        ans /= len(sentence)\n",
    "        return ans\n",
    "    \n",
    "    def words_to_vec(self, df: pd.Series):\n",
    "        words = self.tokenize(df)\n",
    "        df['vec'] = pd.Series(words).progress_apply(self.sen_to_vec)\n",
    "        df = df['vec'].to_numpy()\n",
    "        df = df.reshape(-1, 1)\n",
    "        df = np.concatenate(np.concatenate(df, axis = 0), axis = 0).reshape(-1, 300)\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb4a378",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:23:11.676045Z",
     "start_time": "2022-02-24T12:23:11.659953Z"
    }
   },
   "outputs": [],
   "source": [
    "class Scorer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pipe_count_bigrams = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1, 2))),\n",
    "                            ('lr', LogisticRegression())])\n",
    "        self.pipe_count_lr = Pipeline([('lr', LogisticRegression())])\n",
    "        self.pipe_svc = Pipeline([('svc', LinearSVC(penalty='l1', C=0.55, fit_intercept=False, dual=False, tol=1e-10, max_iter=100000))])\n",
    "        \n",
    "    def data_in_sentences(self, df, labels):\n",
    "        cross_score_bigrams = cross_val_score(\n",
    "            self.pipe_count_bigrams, X_train, y_train, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "        print('(Count Bigram + Unigram Vectorizer + LR) mean score: ', cross_score_bigrams.mean())\n",
    "        \n",
    "    def data_in_vectors(self, df, labels):\n",
    "        cross_score_lr = cross_val_score(\n",
    "            self.pipe_count_lr, df, labels, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "        cross_score_svc = cross_val_score(\n",
    "            self.pipe_svc, df, labels, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "        \n",
    "        print('(Word2vec + LR) mean score: ', cross_score_lr.mean())\n",
    "        print('(Word2vec + SVC) mean score: ', cross_score_svc.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd7f2e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:23:17.880483Z",
     "start_time": "2022-02-24T12:23:11.744389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Count Bigram + Unigram Vectorizer + LR) mean score:  0.7698934837092731\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('products_sentiment_train.tsv', sep='\\t')\n",
    "X_train, y_train = train_df['2 . take around 10,000 640x480 pictures .'], train_df['1']\n",
    "preprocess = Preprocess()\n",
    "scorer = Scorer()\n",
    "scorer.data_in_sentences(X_train, y_train)  # without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3ac407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:23:25.850483Z",
     "start_time": "2022-02-24T12:23:18.000406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Count Bigram + Unigram Vectorizer + LR) mean score:  0.7698934837092731\n"
     ]
    }
   ],
   "source": [
    "X_train_clean = preprocess.lemmatization_and_stop_words(X_train)\n",
    "scorer.data_in_sentences(X_train_clean, y_train)  # just lemmatisation and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b836b13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:28:26.676510Z",
     "start_time": "2022-02-24T12:23:25.990482Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1999/1999 [03:50<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Word2vec + LR) mean score:  0.7918934837092731\n",
      "(Word2vec + SVC) mean score:  0.780889724310777\n"
     ]
    }
   ],
   "source": [
    "X_train_word2vec = preprocess.words_to_vec(X_train)\n",
    "scorer.data_in_vectors(X_train_word2vec, y_train)  # just word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587e51f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:22:50.050661Z",
     "start_time": "2022-02-24T12:19:39.876Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_word2vec_clean = preprocess.words_to_vec(X_train_clean)\n",
    "scorer.data_in_vectors(X_train_word2vec_clean, y_train)  # word2vec with lemmatisation and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2cd3a96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:29:22.706280Z",
     "start_time": "2022-02-24T12:28:26.853435Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:52<00:00,  9.59it/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('products_sentiment_test.tsv', sep='\\t')\n",
    "X_test = test_df['text']\n",
    "X_test_word2vec = preprocess.words_to_vec(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "908cdcf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T12:29:23.050476Z",
     "start_time": "2022-02-24T12:29:22.950258Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_pipeline = Pipeline([('lr', LogisticRegression())])\n",
    "clf_pipeline.fit(X_train_word2vec, y_train)\n",
    "test_df['y'] = clf_pipeline.predict(X_test_word2vec)\n",
    "test_df[['Id','y']].to_csv('sample_sabmission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
